{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rlmCr3lKkBj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, stanza, wordfreq, re, math, tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. read\n",
        "df = pd.read_csv(\"bulwer.tsv\", sep='\\t')"
      ],
      "metadata": {
        "id": "yNV9L4mOKttQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. pipline\n",
        "nlp = stanza.Pipeline(\n",
        "        lang='en',\n",
        "        processors='tokenize,pos,lemma,depparse,constituency',\n",
        "        tokenize_no_ssplit=False,\n",
        "        use_gpu=True\n",
        ")"
      ],
      "metadata": {
        "id": "Y_vvcLhvKyb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. distance\n",
        "def dep_distance(sent):\n",
        "    return sum(abs(w.id - w.head) for w in sent.words[1:]) / (len(sent.words)-1)\n",
        "\n",
        "def tree_depth(const_node):\n",
        "    if not const_node.children:\n",
        "        return 1\n",
        "    return 1 + max(tree_depth(child) for child in const_node.children)"
      ],
      "metadata": {
        "id": "afmTN0gBK5gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Feature\n",
        "records = []\n",
        "for idx, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
        "    doc   = nlp(row['text'])\n",
        "    toks  = [w.text for s in doc.sentences for w in s.words]\n",
        "    lemmas= [w.lemma for s in doc.sentences for w in s.words]\n",
        "    pos   = [w.upos  for s in doc.sentences for w in s.words]\n",
        "\n",
        "# syntactic complexity\n",
        "sent_lens = [len(s.words) for s in doc.sentences]\n",
        "    mean_len  = sum(sent_lens)/len(sent_lens)\n",
        "    cv_len    = (pd.Series(sent_lens).std()/mean_len) if len(sent_lens)>1 else 0\n",
        "    dep_dist  = sum(dep_distance(s) for s in doc.sentences)/len(doc.sentences)\n",
        "    depth     = sum(tree_depth(s.constituency) for s in doc.sentences)/len(doc.sentences)\n",
        "    sub_ratio = sum(1 for p in pos if p in ['SCONJ'])/len(toks)  # 粗略示例\n",
        "    conj_ratio= pos.count('CCONJ')/len(toks)\n",
        "\n",
        "# lexical creativity\n",
        "ttr      = len(set(lemmas))/len(toks)\n",
        "    hapax    = sum(1 for t in set(lemmas) if lemmas.count(t)==1)/len(toks)\n",
        "    avg_len  = sum(len(t) for t in toks)/len(toks)\n",
        "    adj_adv  = sum(1 for p in pos if p in ['ADJ','ADV'])/len(toks)\n",
        "    oov      = sum(1 for t in toks if wordfreq.zipf_frequency(t.lower(),'en')<2)/len(toks)\n",
        "\n",
        "    like_sim = len(re.findall(r'\\blike a\\b|\\blike an\\b', row['text'].lower()))/len(doc.sentences)\n",
        "\n",
        "    records.append({\n",
        "        'genre':   row['genre'],\n",
        "        'mean_sent_len': mean_len,\n",
        "        'cv_sent_len'  : cv_len,\n",
        "        'dep_dist'     : dep_dist,\n",
        "        'tree_depth'   : depth,\n",
        "        'sub_ratio'    : sub_ratio,\n",
        "        'conj_ratio'   : conj_ratio,\n",
        "        'ttr'          : ttr,\n",
        "        'hapax'        : hapax,\n",
        "        'avg_token_len': avg_len,\n",
        "        'adj_adv_ratio': adj_adv,\n",
        "        'oov_ratio'    : oov,\n",
        "        'simile_rate'  : like_sim\n",
        "    })\n",
        "\n",
        "feat_df = pd.DataFrame(records)\n"
      ],
      "metadata": {
        "id": "phCmYRltLGvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Genre\n",
        "# feature\n",
        "df[\"syntactic_complexity\"] = df[\"text\"].apply(get_syntactic_complexity)\n",
        "df[\"lexical_creativity\"] = df[\"text\"].apply(get_lexical_creativity)\n",
        "\n",
        "# genre\n",
        "complexity_by_genre = df.groupby(\"genre\")[\"syntactic_complexity\"].apply(lambda x: pd.DataFrame(x.tolist()).mean())\n",
        "creativity_by_genre = df.groupby(\"genre\")[\"lexical_creativity\"].apply(lambda x: pd.DataFrame(x.tolist()).mean())\n",
        "\n",
        "print(\"Syntactic Complexity by Genre:\")\n",
        "print(complexity_by_genre)\n",
        "print(\"\\nLexical Creativity by Genre:\")\n",
        "print(creativity_by_genre)"
      ],
      "metadata": {
        "id": "mXZDhcQsL60r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 示例：可视化依存深度\n",
        "sns.boxplot(data=df, x=\"genre\", y=\"syntactic_complexity.dependency_depth\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Dependency Depth by Genre\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gEe1o46gOWnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}