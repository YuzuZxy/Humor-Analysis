# -*- coding: utf-8 -*-
"""Stanza.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bGarC9XnzZHxOA8E0-OEHYNb_vq2fwiw
"""

from __future__ import annotations

import csv
import re
from pathlib import Path
from typing import Dict, Any

import pandas as pd
import stanza
from tqdm.auto import tqdm

try:
    from wordfreq import zipf_frequency  # for rare-word ratio
except ImportError:                      # make it optional
    zipf_frequency = lambda w, lang: 5.0  # ≈ neutral frequency

import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

!pip install -q stanza
!pip install -q wordfreq
!pip install -q seaborn matplotlib

import pandas as pd
import stanza
stanza.download("en")
import re
from typing import Dict, List, Any, Optional
from pathlib import Path
import csv
import seaborn as sns
import matplotlib.pyplot as plt
from wordfreq import zipf_frequency

import logging
from typing import Dict, Any

class BulwerLyttonAnalyzer:
    """Encapsulates all analysis logic in a reusable class"""

    def __init__(self, data_path: str | Path, *, gpu: bool = False):
        self.df: pd.DataFrame = self.load_data(data_path)
        self.nlp = self._init_nlp(gpu=gpu)
        self._simile_re = re.compile(r"\b(?:like|as)\b", flags=re.I)

    # 1. read
    def load_data(self, filepath: str) -> pd.DataFrame:
      # Load and clean TSV data
      df = pd.read_csv(filepath,
                       sep="\t",
                       quoting=csv.QUOTE_NONE,
                       on_bad_lines="skip",
                       keep_default_na=False
                       )
      expected = ["year", "text", "genre", "author", "location", "winning"]
      df.columns = expected[: len(df.columns)]
      df["genre"] = df["genre"].fillna("General")
      df["text"] = df["text"].astype(str)
      return df

    # 2. pipline
    def _init_nlp(self, *, gpu: bool = False) -> stanza.Pipeline:
      # Initialize Stanza
      return stanza.Pipeline(
            lang="en",
            processors="tokenize,pos,lemma,depparse,ner",
            use_gpu=gpu,
            tokenize_pretokenized=False,
            verbose=False,
            download_method=None
        )

    # 3. Feature
    # syntax
    @staticmethod
    def _tree_depth(sentence: stanza.models.common.doc.Sentence) -> int:
        """Return max depth from any node to ROOT (inclusive)."""
        # build parent dict once:
        parents = {word.id: word.head for word in sentence.words}
        max_depth = 0
        for wid in parents:
            d = 0
            while wid != 0:
                wid = parents[wid]
                d += 1
            max_depth = max(max_depth, d)
        return max_depth

    @staticmethod
    def _dependency_distance(sentence: stanza.models.common.doc.Sentence) -> float:
        """Mean absolute distance head<->dep."""
        dists = [abs(w.id - w.head) for w in sentence.words if w.head != 0]
        return sum(dists) / len(dists) if dists else 0.0

    _FEATURE_KEYS = [
        # syntactic
        "tree_depth", "dep_distance", "clause_ratio",
        "conj_count", "avg_sent_len",
        # lexical
        "ttr", "rare_ratio", "ner_density", "pos_diversity",
        # rhetoric
        "simile_density", "adj_count"
    ]

    def _zero_feat(self) -> Dict[str, float]:
        """dict with feature 0"""
        return {k: 0.0 for k in self._FEATURE_KEYS}

    def _extract_features(self, row: pd.Series) -> Dict[str, Any]:
        """Main feature extraction router"""
        text = str(row.get("text", "")).strip()
        if not text:
            return self._zero_feat()
        try:
            doc: stanza.Document = self.nlp(text)
        except Exception as e:
            logging.warning(
                "Stanza failed on row %s (%s) — %s",
                row.name, text[:40] + ("…" if len(text) > 40 else ""), e
            )
            return self._zero_feat()

        # Flatten tokens
        tokens = [w.text for s in doc.sentences for w in s.words]
        n_tokens = len(tokens)
        if n_tokens == 0:
            return self._zero_feat()

        feats = {}
        feats.update(self._extract_syntax_features(doc, n_tokens))
        feats.update(self._extract_lexical_features(doc, tokens))
        feats.update(self._extract_rhetoric_features(text, doc, n_tokens))
        return feats

    def _extract_syntax_features(self, doc: stanza.Document, n_tokens: int) -> Dict[str, float]:
        """Calculate syntactic complexity metrics"""
        if not doc.sentences:
            return {
                "tree_depth": 0.0,
                "dep_distance": 0.0,
                "clause_ratio": 0.0,
                "conj_count": 0.0,
                "avg_sent_len": 0.0
            }

        # Sentence-level metrics
        depths = []
        dep_dists = []
        sent_lengths = []
        n_clauses = 0
        n_conj = 0
        n_deprel = 0

        for sent in doc.sentences:
            depths.append(self._tree_depth(sent))
            dep_dists.append(self._dependency_distance(sent))
            sent_lengths.append(len(sent.words))

            for word in sent.words:
                if word.deprel in {"acl", "advcl", "relcl"}:
                    n_clauses += 1
                elif word.deprel == "conj":
                    n_conj += 1

            n_deprel += len(sent.words)

        return {
            "tree_depth": max(depths),
            "dep_distance": sum(dep_dists) / len(dep_dists),
            "clause_ratio": n_clauses / n_deprel if n_deprel else 0.0,
            "conj_count": n_conj,
            "avg_sent_len": sum(sent_lengths) / len(sent_lengths)
        }

    def _extract_lexical_features(self, doc: stanza.Document, tokens: List[str]) -> Dict[str, float]:
        """Calculate lexical creativity metrics"""
        if not tokens:
            return {
                "ttr": 0.0,
                "rare_ratio": 0.0,
                "ner_density": 0.0,
                "pos_diversity": 0.0
            }

        # Unique words
        unique_tokens = set(tokens)

        # Rare words (Zipf frequency < 3)
        rare_words = sum(1 for w in tokens if zipf_frequency(w, "en") < 3.0)

        # POS diversity
        pos_tags = [w.upos for s in doc.sentences for w in s.words]

        # NER density
        n_ents = len(doc.ents)

        return {
            "ttr": len(unique_tokens) / len(tokens),
            "rare_ratio": rare_words / len(tokens),
            "ner_density": n_ents / len(tokens),
            "pos_diversity": len(set(pos_tags)) / len(pos_tags) if pos_tags else 0.0
        }

    def _extract_rhetoric_features(self, text: str, doc: stanza.Document, n_tokens: int) -> Dict[str, float]:
        """Detect rhetorical devices"""
        if not doc.sentences:
            return {
                "simile_density": 0.0,
                "adj_count": 0.0
            }

        # Adjectives
        n_adj = sum(1 for w in doc.iter_words() if w.xpos and w.xpos.startswith("JJ"))

        # Similes
        n_like_as = len(self._simile_re.findall(text.lower()))

        return {
            "simile_density": (n_adj + n_like_as) / n_tokens if n_tokens else 0.0,
            "adj_count": n_adj
        }

    # 4. Analysis Methods
    # ----------------------------
    def analyze(self, sample_size: Optional[int] = None) -> None:
        """Run full analysis pipeline"""
        if sample_size is None:
          self.df = self.df.reset_index(drop=True)
        else:
          n = min(sample_size, len(self.df))
          self.df = (self.df.sample(n, random_state=42)
                         .reset_index(drop=True))

        # Apply feature extraction
        feature_dicts = self.df.apply(self._extract_features, axis=1).tolist()
        features_df   = pd.DataFrame(feature_dicts)

        dup_cols = [c for c in features_df.columns if c in self.df.columns]
        self.df.drop(columns=dup_cols, inplace=True, errors="ignore")
        self.df = pd.concat([self.df, features_df], axis=1)
        self.df = self.df.loc[:, ~self.df.columns.duplicated()].copy()

    def genre_summary(self) -> pd.DataFrame:
        """Statistical summary by genre"""
        num_cols = self.df.select_dtypes(include=['number']).columns
        return self.df.groupby("genre")[num_cols].agg(["mean", "std"])

    def year_summary(self, stats: tuple[str, ...] = ("mean", "std", "count")
                     ) -> pd.DataFrame:
        """Descriptive statistics for every numeric feature **per year**."""
        num_cols = self.df.select_dtypes(include="number").columns
        return self.df.groupby("year")[num_cols].agg(stats)

    def plot_feature(self, feature: str, *, kind: str = "violin",
                     by: str = "genre", order: Optional[List[str]] = None) -> None:
        """Visualize feature distribution by genre or year"""
        x_col = by if by in {"genre", "year"} else "genre"
        if x_col == "year":
            self.df["year_str"] = self.df["year"].astype(str)

        plt.figure(figsize=(10, 5))
        if kind == "box":
            sns.boxplot(data=self.df, x=x_col, y=feature, order=order)
        elif kind == "swarm":
            sns.swarmplot(data=self.df, x=x_col, y=feature, order=order, size=3)
        else:
            sns.violinplot(data=self.df, x=x_col, y=feature, order=order,
                           inner="quartile")
        plt.title(f"{feature} by {x_col.capitalize()}")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

from google.colab import drive
drive.mount('/content/drive')

if __name__ == "__main__":

    !ls -R /content/drive/MyDrive | grep bulwer
    file_path = "/content/drive/MyDrive/bulwer.tsv"
    analyzer = BulwerLyttonAnalyzer(file_path, gpu=True)

    # Analyze first 100 entries
    analyzer.analyze(sample_size=100)

    # Show summary
    print(analyzer.genre_summary())
    year_stats = analyzer.year_summary()
    print(year_stats)
    year_stats.to_csv("/content/drive/MyDrive/bulwer_year_summary.csv")

    # Visualize key features
    analyzer.plot_feature("tree_depth")
    analyzer.plot_feature("tree_depth", by="year", kind="box")
    analyzer.plot_feature("ttr", kind="box")
    analyzer.plot_feature("ttr", kind="box",  by="year")
    analyzer.plot_feature("simile_density")
    analyzer.plot_feature("simile_density", by="year")

class BulwerLyttonAnalyzer:
    """Encapsulates all analysis logic in a reusable class"""

    def __init__(self, data_path: str | Path, *, gpu: bool = False):
        self.df: pd.DataFrame = self.load_data(data_path)
        self.nlp = self._init_nlp(gpu=gpu)
        self._simile_re = re.compile(r"\b(?:like|as)\b", flags=re.I)

    # 1. read
    def load_data(self, filepath: str) -> pd.DataFrame:
      # Load and clean TSV data
      df = pd.read_csv(filepath,
                       sep="\t",
                       quoting=csv.QUOTE_NONE,
                       on_bad_lines="skip",
                       keep_default_na=False
                       )
      expected = ["year", "text", "genre", "author", "location", "winning"]
      df.columns = expected[: len(df.columns)]
      df["genre"] = df["genre"].fillna("General")
      df["text"] = df["text"].astype(str)
      return df

    # 2. pipline
    def _init_nlp(self, *, gpu: bool = False) -> stanza.Pipeline:
      # Initialize Stanza
      return stanza.Pipeline(
            lang="en",
            processors="tokenize,pos,lemma,depparse,ner",
            use_gpu=gpu,
            tokenize_pretokenized=False,
            verbose=False,
            download_method=None
        )

    # 3. Feature
    # syntax
    @staticmethod
    def _tree_depth(sentence: stanza.models.common.doc.Sentence) -> int:
        """Return max depth from any node to ROOT (inclusive)."""
        # build parent dict once:
        parents = {word.id: word.head for word in sentence.words}
        max_depth = 0
        for wid in parents:
            d = 0
            while wid != 0:
                wid = parents[wid]
                d += 1
            max_depth = max(max_depth, d)
        return max_depth

    @staticmethod
    def _dependency_distance(sentence: stanza.models.common.doc.Sentence) -> float:
        """Mean absolute distance head<->dep."""
        dists = [abs(w.id - w.head) for w in sentence.words if w.head != 0]
        return sum(dists) / len(dists) if dists else 0.0

    _FEATURE_KEYS = [
        # syntactic
        "tree_depth", "dep_distance", "clause_ratio",
        "conj_count", "avg_sent_len",
        # lexical
        "ttr", "rare_ratio", "ner_density", "pos_diversity",
        # rhetoric
        "simile_density", "adj_count"
    ]

    def _zero_feat(self) -> Dict[str, float]:
        """dict with feature 0"""
        return {k: 0.0 for k in self._FEATURE_KEYS}

    def _extract_features(self, row: pd.Series) -> Dict[str, Any]:
        """Main feature extraction router"""
        text = str(row.get("text", "")).strip()
        if not text:
            return self._zero_feat()
        try:
            doc: stanza.Document = self.nlp(text)
        except Exception as e:
            logging.warning(
                "Stanza failed on row %s (%s) — %s",
                row.name, text[:40] + ("…" if len(text) > 40 else ""), e
            )
            return self._zero_feat()

        # Flatten tokens
        tokens = [w.text for s in doc.sentences for w in s.words]
        n_tokens = len(tokens)
        if n_tokens == 0:
            return self._zero_feat()

        feats = {}
        feats.update(self._extract_syntax_features(doc, n_tokens))
        feats.update(self._extract_lexical_features(doc, tokens))
        feats.update(self._extract_rhetoric_features(text, doc, n_tokens))
        return feats

    def _extract_syntax_features(self, doc: stanza.Document, n_tokens: int) -> Dict[str, float]:
        """Calculate syntactic complexity metrics"""
        if not doc.sentences:
            return {
                "tree_depth": 0.0,
                "dep_distance": 0.0,
                "clause_ratio": 0.0,
                "conj_count": 0.0,
                "avg_sent_len": 0.0
            }

        # Sentence-level metrics
        depths = []
        dep_dists = []
        sent_lengths = []
        n_clauses = 0
        n_conj = 0
        n_deprel = 0

        for sent in doc.sentences:
            depths.append(self._tree_depth(sent))
            dep_dists.append(self._dependency_distance(sent))
            sent_lengths.append(len(sent.words))

            for word in sent.words:
                if word.deprel in {"acl", "advcl", "relcl"}:
                    n_clauses += 1
                elif word.deprel == "conj":
                    n_conj += 1

            n_deprel += len(sent.words)

        return {
            "tree_depth": max(depths),
            "dep_distance": sum(dep_dists) / len(dep_dists),
            "clause_ratio": n_clauses / n_deprel if n_deprel else 0.0,
            "conj_count": n_conj,
            "avg_sent_len": sum(sent_lengths) / len(sent_lengths)
        }

    def _extract_lexical_features(self, doc: stanza.Document, tokens: List[str]) -> Dict[str, float]:
        """Calculate lexical creativity metrics"""
        if not tokens:
            return {
                "ttr": 0.0,
                "rare_ratio": 0.0,
                "ner_density": 0.0,
                "pos_diversity": 0.0
            }

        # Unique words
        unique_tokens = set(tokens)

        # Rare words (Zipf frequency < 3)
        rare_words = sum(1 for w in tokens if zipf_frequency(w, "en") < 3.0)

        # POS diversity
        pos_tags = [w.upos for s in doc.sentences for w in s.words]

        # NER density
        n_ents = len(doc.ents)

        return {
            "ttr": len(unique_tokens) / len(tokens),
            "rare_ratio": rare_words / len(tokens),
            "ner_density": n_ents / len(tokens),
            "pos_diversity": len(set(pos_tags)) / len(pos_tags) if pos_tags else 0.0
        }

    def _extract_rhetoric_features(self, text: str, doc: stanza.Document, n_tokens: int) -> Dict[str, float]:
        """Detect rhetorical devices"""
        if not doc.sentences:
            return {
                "simile_density": 0.0,
                "adj_count": 0.0
            }

        # Adjectives
        n_adj = sum(1 for w in doc.iter_words() if w.xpos and w.xpos.startswith("JJ"))

        # Similes
        n_like_as = len(self._simile_re.findall(text.lower()))

        return {
            "simile_density": (n_adj + n_like_as) / n_tokens if n_tokens else 0.0,
            "adj_count": n_adj
        }

    # 4. Analysis Methods
    # ----------------------------
    def analyze(self, sample_size: Optional[int] = None) -> None:
        """Run full analysis pipeline"""
        if sample_size is None:
          self.df = self.df.reset_index(drop=True)
        else:
          n = min(sample_size, len(self.df))
          self.df = (self.df.sample(n, random_state=42)
                         .reset_index(drop=True))

        # Apply feature extraction
        feature_dicts = self.df.apply(self._extract_features, axis=1).tolist()
        features_df   = pd.DataFrame(feature_dicts)

        dup_cols = [c for c in features_df.columns if c in self.df.columns]
        self.df.drop(columns=dup_cols, inplace=True, errors="ignore")
        self.df = pd.concat([self.df, features_df], axis=1)
        self.df = self.df.loc[:, ~self.df.columns.duplicated()].copy()

    def genre_summary(self) -> pd.DataFrame:
        """Statistical summary by genre"""
        num_cols = self.df.select_dtypes(include=['number']).columns
        return self.df.groupby("genre")[num_cols].agg(["mean", "std"])

    def plot_feature(
            self,
            feature: str,
            kind: str = "violin",
            by: str = "genre",
            order: Optional[List[str]] = None,
        ) -> None:
        """Visualize feature distribution by genre"""
        plt.figure(figsize=(10, 6))

        if by not in self.df.columns:
            raise ValueError(f"Column '{by}' not in DataFrame.")

        data = self.df.copy()
        if order is None and by == "year":
            order = sorted(data["year"].unique())

        if kind == "box":
            sns.boxplot(data=data, x=by, y=feature, order=order)
        elif kind == "swarm":
            sns.swarmplot(data=data, x=by, y=feature, order=order, size=3)
        else:
            sns.violinplot(data=data, x=by, y=feature, order=order, inner="quartile")

        plt.title(f"{feature} by {by.capitalize()}")
        plt.xticks(rotation=45 if by == "genre" else 90)
        plt.tight_layout()
        plt.show()

from google.colab import drive
drive.mount('/content/drive')

if __name__ == "__main__":

    !ls -R /content/drive/MyDrive | grep bulwer
    file_path = "/content/drive/MyDrive/bulwer.tsv"
    analyzer = BulwerLyttonAnalyzer("/content/drive/MyDrive/bulwer.tsv", gpu=True)
    analyzer.analyze()

    analyzer.df['genre'] = analyzer.df['genre'].str.strip().str.title()
    major_genres = analyzer.df['genre'].value_counts().head(10).index.tolist()
    analyzer.df = analyzer.df[analyzer.df['genre'].isin(major_genres)]

    from scipy.stats import kruskal
    groups = [g['tree_depth'].dropna() for _, g in analyzer.df.groupby('genre')]
    H, p = kruskal(*groups)
    print(f"Kruskal-Wallis H={H:.2f}, p={p:.3g}")

    # Show summary
    print(analyzer.genre_summary())

    # Visualize key features
    analyzer.plot_feature("tree_depth")
    analyzer.plot_feature("ttr", kind="box")
    analyzer.plot_feature("simile_density")
    analyzer.plot_feature("tree_depth", by="year", kind="box")
    analyzer.plot_feature("ttr", kind="box",  by="year")
    analyzer.plot_feature("simile_density", by="year")

!pip install -q scikit-posthocs

import scikit_posthocs as sp
p_matrix = sp.posthoc_dunn(analyzer.df, val_col='tree_depth', group_col='genre', p_adjust='fdr_bh')
print(p_matrix)